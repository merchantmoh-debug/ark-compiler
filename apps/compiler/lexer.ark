
// The Architect: Functional Lexer Implementation
// Objective: Convert source string into a list of Tokens using Structs.

// --- Constants ---
// Token Types
TOKEN_EOF := 0
TOKEN_IDENTIFIER := 1
TOKEN_NUMBER := 2
TOKEN_STRING := 3
TOKEN_SYMBOL := 4
TOKEN_KEYWORD := 5

// Keywords (Must match Parser)
TOKEN_KEYWORD_IF := 100
TOKEN_KEYWORD_ELSE := 101
TOKEN_KEYWORD_WHILE := 102
TOKEN_KEYWORD_FUNC := 103
TOKEN_KEYWORD_RETURN := 104
TOKEN_KEYWORD_LET := 105

// --- Helpers ---

func is_digit(char_str) {
    return (char_str >= "0") and (char_str <= "9")
}

func is_alpha(char_str) {
    return ((char_str >= "a") and (char_str <= "z")) or ((char_str >= "A") and (char_str <= "Z")) or (char_str == "_")
}

func is_whitespace(char_str) {
    return (char_str == " ") or (char_str == "\n") or (char_str == "\t") or (char_str == "\r")
}

// --- Lexer Struct & Methods ---

func lexer_new(source) {
    let (l, src) := sys.len(source)
    return {
        source: src,
        length: l,
        pos: 0,
        line: 1,
        col: 1,
        tokens: []
    }
}

func lexer_peek(lexer) {
    if lexer.pos >= lexer.length {
        return "" // EOF
    }
    let (c, src) := sys.str.get(lexer.source, lexer.pos)
    return c
}

func lexer_advance(lexer) {
    c := lexer_peek(lexer)
    lexer.pos := lexer.pos + 1
    if c == "\n" {
        lexer.line := lexer.line + 1
        lexer.col := 1
    } else {
        lexer.col := lexer.col + 1
    }
    // Return modified lexer because we modified fields in place?
    // `lexer.pos := ...` updates the Struct in place (Rust `eval.rs` SetField).
    return lexer
}

func lexer_add_token(lexer, type, value) {
    token := {
        type: type,
        value: value,
        line: lexer.line,
        col: lexer.col
    }
    lexer.tokens := sys.list.append(lexer.tokens, token)
    return lexer
}

func check_keyword(ident_str) {
    if ident_str == "if" { return 100 }
    if ident_str == "else" { return 101 }
    if ident_str == "while" { return 102 }
    if ident_str == "func" { return 103 }
    if ident_str == "return" { return 104 }
    if ident_str == "let" { return 105 }
    return 1 // TOKEN_IDENTIFIER
}

func lexer_scan(lexer) {
    // Main loop
    char_str := lexer_peek(lexer)
    
    if char_str == "" {
        lexer := lexer_add_token(lexer, TOKEN_EOF, "EOF")
        return lexer
    }
    
    if is_whitespace(char_str) {
        lexer := lexer_advance(lexer)
        return lexer_scan(lexer) // Tail recursion? We don't have TCO. Loop in evaluator.
        // Wait, Ark doesn't have loops in functions yet?
        // It has `while` statement.
        // So `lexer_scan` should be called in a loop in `main`.
    }
    
    if is_digit(char_str) {
        // Parse number
        num_str := ""
        while is_digit(lexer_peek(lexer)) {
            num_str := num_str + lexer_peek(lexer)
            lexer := lexer_advance(lexer)
        }
        lexer := lexer_add_token(lexer, TOKEN_NUMBER, num_str)
        return lexer
    }
    

    if is_alpha(char_str) {
        // Parse identifier/keyword
        ident_str := ""
        while is_alpha(lexer_peek(lexer)) or is_digit(lexer_peek(lexer)) {
            ident_str := ident_str + lexer_peek(lexer)
            lexer := lexer_advance(lexer)
        }
        
        token_type := check_keyword(ident_str)
        lexer := lexer_add_token(lexer, token_type, ident_str)
        return lexer
    }

    // Comments
    if char_str == "/" {
        if (lexer.pos + 1) < lexer.length {
            let (next_char, _src) := sys.str.get(lexer.source, lexer.pos + 1)
            if next_char == "/" {
                 // Skip until newline
                 looping := true
                 while looping {
                     lexer := lexer_advance(lexer)
                     c := lexer_peek(lexer)
                     if c == "\n" {
                         looping := false
                     }
                     if c == "" {
                         looping := false
                     }
                 }
                 return lexer_scan(lexer)
            }
        }
    }
    
    // Symbols
    lexer := lexer_add_token(lexer, TOKEN_SYMBOL, char_str)
    lexer := lexer_advance(lexer)
    return lexer
}

func lexer_tokenize(source) {
    lexer := lexer_new(source)
    running := true
    while running {
        let (count, tokens_ref) := sys.len(lexer.tokens)
        lexer.tokens := tokens_ref
        if count > 0 {
            last_token := sys.list.get(lexer.tokens, count - 1)
            let (token_val, token_list) := last_token
            type := token_val.type
            if type == TOKEN_EOF {
                running := false
            } else {
                lexer := lexer_scan(lexer)
            }
        } else {
            lexer := lexer_scan(lexer)
        }
    }
    return lexer.tokens
}
