
// The Architect: Functional Lexer Implementation
// Objective: Convert source string into a list of Tokens using Structs.

// --- Constants ---
true := 1
false := 0

// Token Types
TOKEN_EOF := 0
TOKEN_IDENTIFIER := 1
TOKEN_NUMBER := 2
TOKEN_STRING := 3
TOKEN_SYMBOL := 4 // Generic Symbol or EQ (=)
TOKEN_KEYWORD := 5

// Specific Symbols (Must match Parser)
TOKEN_EQ := 4         // = (Aliased to SYMBOL for now if generic)
TOKEN_PLUS := 5       // +
TOKEN_MINUS := 6      // -
TOKEN_STAR := 7       // *
TOKEN_SLASH := 8      // /
TOKEN_LPAREN := 9     // (
TOKEN_RPAREN := 10    // )
TOKEN_LBRACE := 11    // {
TOKEN_RBRACE := 12    // }
TOKEN_COMMA := 13     // ,
TOKEN_COLON := 14     // :
TOKEN_SEMICOLON := 15 // ;
TOKEN_ASSIGN := 16    // :=

// Keywords (Must match Parser)
TOKEN_KEYWORD_IF := 100
TOKEN_KEYWORD_ELSE := 101
TOKEN_KEYWORD_WHILE := 102
TOKEN_KEYWORD_FUNC := 103
TOKEN_KEYWORD_RETURN := 104
TOKEN_KEYWORD_LET := 105

// --- Helpers ---

func is_digit(char_str) {
    return (char_str >= "0") and (char_str <= "9")
}

func is_alpha(char_str) {
    return ((char_str >= "a") and (char_str <= "z")) or ((char_str >= "A") and (char_str <= "Z")) or (char_str == "_")
}

func is_whitespace(char_str) {
    return (char_str == " ") or (char_str == "\n") or (char_str == "\t") or (char_str == "\r")
}

// --- Lexer Struct & Methods ---

func lexer_new(source) {
    let (l, src) := sys.len(source)
    return {
        source: src,
        length: l,
        pos: 0,
        line: 1,
        col: 1,
        tokens: []
    }
}

func lexer_peek(lexer) {
    if lexer.pos >= lexer.length {
        return "" // EOF
    }
    let (c, src) := sys.str.get(lexer.source, lexer.pos)
    return c
}

func lexer_advance(lexer) {
    c := lexer_peek(lexer)
    lexer.pos := lexer.pos + 1
    if c == "\n" {
        lexer.line := lexer.line + 1
        lexer.col := 1
    } else {
        lexer.col := lexer.col + 1
    }
    return lexer
}

func lexer_add_token(lexer, type, value) {
    token := {
        type: type,
        value: value,
        line: lexer.line,
        col: lexer.col
    }
    lexer.tokens := sys.list.append(lexer.tokens, token)
    return lexer
}

func check_keyword(ident_str) {
    if ident_str == "if" { return 100 }
    if ident_str == "else" { return 101 }
    if ident_str == "while" { return 102 }
    if ident_str == "func" { return 103 }
    if ident_str == "return" { return 104 }
    if ident_str == "let" { return 105 }
    return 1 // TOKEN_IDENTIFIER
}

func lexer_skip_comment(lexer) {
    looping := true
    while looping {
        lexer := lexer_advance(lexer)
        c := lexer_peek(lexer)
        if (c == "\n") or (c == "") {
            looping := false
        }
    }
    return lexer
}

func lexer_scan(lexer) {
    // Main loop
    char_str := lexer_peek(lexer)
    
    if char_str == "" {
        lexer := lexer_add_token(lexer, TOKEN_EOF, "EOF")
        return lexer
    }
    
    if is_whitespace(char_str) {
        lexer := lexer_advance(lexer)
        return lexer_scan(lexer)
    }
    
    if is_digit(char_str) {
        // Parse number
        num_str := ""
        while is_digit(lexer_peek(lexer)) {
            num_str := num_str + lexer_peek(lexer)
            lexer := lexer_advance(lexer)
        }
        lexer := lexer_add_token(lexer, TOKEN_NUMBER, num_str)
        return lexer
    }
    

    if is_alpha(char_str) {
        // Parse identifier/keyword
        ident_str := ""
        while is_alpha(lexer_peek(lexer)) or is_digit(lexer_peek(lexer)) {
            ident_str := ident_str + lexer_peek(lexer)
            lexer := lexer_advance(lexer)
        }
        
        token_type := check_keyword(ident_str)
        lexer := lexer_add_token(lexer, token_type, ident_str)
        return lexer
    }

    // Comments & Slash
    if char_str == "/" {
        if (lexer.pos + 1) < lexer.length {
            let (next_char, _src) := sys.str.get(lexer.source, lexer.pos + 1)
            if next_char == "/" {
                 lexer := lexer_skip_comment(lexer)
                 return lexer_scan(lexer)
            }
        }
        // Not a comment, it is slash
        lexer := lexer_add_token(lexer, TOKEN_SLASH, "/")
        lexer := lexer_advance(lexer)
        return lexer
    }
    
    // Symbols
    type := TOKEN_SYMBOL
    val := char_str
    advanced := 0

    if char_str == "+" { type := 5 }
    if char_str == "-" { type := 6 }
    if char_str == "*" { type := 7 }
    // / handled above
    if char_str == "(" { type := 9 }
    if char_str == ")" { type := 10 }
    if char_str == "{" { type := 11 }
    if char_str == "}" { type := 12 }
    if char_str == "," { type := 13 }
    if char_str == ":" {
         // Check :=
         if (lexer.pos + 1) < lexer.length {
             let (next, _) := sys.str.get(lexer.source, lexer.pos + 1)
             if next == "=" {
                 type := 16
                 val := ":="
                 advanced := 1
             } else { type := 14 }
         } else { type := 14 }
    }
    if char_str == ";" { type := 15 }
    if char_str == "=" { type := 4 } // EQ

    lexer := lexer_add_token(lexer, type, val)
    lexer := lexer_advance(lexer)
    if advanced == 1 {
        lexer := lexer_advance(lexer)
    }
    return lexer
}

func lexer_tokenize(source) {
    lexer := lexer_new(source)
    running := true
    while running {
        let (count, tokens_ref) := sys.len(lexer.tokens)
        lexer.tokens := tokens_ref
        if count > 0 {
            last_token := sys.list.get(lexer.tokens, count - 1)
            let (token_val, token_list) := last_token
            type := token_val.type
            if type == TOKEN_EOF {
                running := false
            } else {
                lexer := lexer_scan(lexer)
            }
        } else {
            lexer := lexer_scan(lexer)
        }
    }
    return lexer.tokens
}
