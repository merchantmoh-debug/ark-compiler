
// The Architect: Functional Lexer Implementation & Test Runner
// Objective: Convert source string into a list of Tokens using Structs.

// --- Constants ---
true := 1
false := 0

// Token Types
TOKEN_EOF := 0
TOKEN_IDENTIFIER := 1
TOKEN_NUMBER := 2
TOKEN_STRING := 3
TOKEN_SYMBOL := 4
TOKEN_KEYWORD := 5

// Specific Keywords
TOKEN_KEYWORD_IF := 100
TOKEN_KEYWORD_ELSE := 101
TOKEN_KEYWORD_WHILE := 102
TOKEN_KEYWORD_FUNC := 103
TOKEN_KEYWORD_RETURN := 104
TOKEN_KEYWORD_LET := 105
TOKEN_KEYWORD_CLASS := 106
TOKEN_KEYWORD_TRUE := 107
TOKEN_KEYWORD_FALSE := 108
TOKEN_AND := 23
TOKEN_OR := 24

// --- Helpers ---

func is_digit(char_str) {
    return (char_str >= "0") && (char_str <= "9")
}

func is_alpha(char_str) {
    return ((char_str >= "a") && (char_str <= "z")) || ((char_str >= "A") && (char_str <= "Z")) || (char_str == "_")
}

func is_whitespace(char_str) {
    return (char_str == " ") || (char_str == "\n") || (char_str == "\t") || (char_str == "\r")
}

func check_keyword(ident_str) {
    if ident_str == "if" { return TOKEN_KEYWORD_IF }
    if ident_str == "else" { return TOKEN_KEYWORD_ELSE }
    if ident_str == "while" { return TOKEN_KEYWORD_WHILE }
    if ident_str == "func" { return TOKEN_KEYWORD_FUNC }
    if ident_str == "return" { return TOKEN_KEYWORD_RETURN }
    if ident_str == "let" { return TOKEN_KEYWORD_LET }
    if ident_str == "and" { return TOKEN_AND }
    if ident_str == "or" { return TOKEN_OR }
    if ident_str == "class" { return TOKEN_KEYWORD_CLASS }
    if ident_str == "true" { return TOKEN_KEYWORD_TRUE }
    if ident_str == "false" { return TOKEN_KEYWORD_FALSE }
    return TOKEN_IDENTIFIER
}

// --- Lexer Struct & Methods ---

func lexer_new(source) {
    let (l, src) := sys.len(source)
    return {
        source: src,
        length: l,
        pos: 0,
        line: 1,
        col: 1,
        tokens: [],
        token_count: 0
    }
}

func lexer_peek(lexer) {
    if lexer.pos >= lexer.length {
        return "" // EOF
    }
    let (c, src) := sys.str.get(lexer.source, lexer.pos)
    return c
}

func lexer_advance(lexer) {
    c := lexer_peek(lexer)
    lexer.pos := lexer.pos + 1
    if c == "\n" {
        lexer.line := lexer.line + 1
        lexer.col := 1
    } else {
        lexer.col := lexer.col + 1
    }
    return lexer
}

func lexer_add_token(lexer, type, value) {
    token := {
        type: type,
        value: value,
        line: lexer.line,
        col: lexer.col
    }

    lexer.tokens := sys.list.append(lexer.tokens, token)
    lexer.token_count := lexer.token_count + 1
    return lexer
}

func lexer_skip_comment(lexer) {
    looping := true
    while looping {
        lexer := lexer_advance(lexer)
        c := lexer_peek(lexer)
        if (c == "\n") || (c == "") {
            looping := false
        }
    }
    return lexer
}

func lexer_scan(lexer) {
    // Main loop
    char_str := lexer_peek(lexer)
    
    if char_str == "" {
        lexer := lexer_add_token(lexer, TOKEN_EOF, "EOF")
        return lexer
    }
    
    if is_whitespace(char_str) {
        lexer := lexer_advance(lexer)
        return lexer_scan(lexer) // Tail recursion simulation (manual recursion)
    }
    
    if is_digit(char_str) {
        // Parse number
        num_str := ""
        // While loop condition needs careful handling of peek
        // In Ark, while condition is expression.
        // We need a helper to check safely without advancing or errors?
        // simple while loop:
        
        looping := true
        while looping {
             c := lexer_peek(lexer)
             if is_digit(c) {
                 num_str := num_str + c
                 lexer := lexer_advance(lexer)
             } else {
                 looping := false
             }
        }
        lexer := lexer_add_token(lexer, TOKEN_NUMBER, num_str)
        return lexer
    }
    
    if is_alpha(char_str) {
        // Parse identifier/keyword
        ident_str := ""
        looping := true
        while looping {
             c := lexer_peek(lexer)
             if is_alpha(c) || is_digit(c) {
                 ident_str := ident_str + c
                 lexer := lexer_advance(lexer)
             } else {
                 looping := false
             }
        }
        // Properly check keywords
        type := check_keyword(ident_str)

        lexer := lexer_add_token(lexer, type, ident_str)
        return lexer
    }

    // Comments
    if char_str == "/" {
        if (lexer.pos + 1) < lexer.length {
            let (next_char, _src) := sys.str.get(lexer.source, lexer.pos + 1)
            if next_char == "/" {
                 lexer := lexer_skip_comment(lexer)
                 return lexer_scan(lexer)
            }
        }
    }
    
    // Symbols (single char for now)
    // Handle := (2 chars)
    if char_str == ":" {
         // Check next
         // Peek next char requires lookahead logic not in `peek`.
         // `source[pos+1]`
         next_char := ""
         if (lexer.pos + 1) < lexer.length {
             next_char := lexer.source[lexer.pos + 1]
         }
         
         if next_char == "=" {
             lexer := lexer_add_token(lexer, TOKEN_SYMBOL, ":=")
             lexer := lexer_advance(lexer)
             lexer := lexer_advance(lexer)
             return lexer
         }
    }

    lexer := lexer_add_token(lexer, TOKEN_SYMBOL, char_str)
    lexer := lexer_advance(lexer)
    return lexer
}

func lexer_tokenize(source) {
    lexer := lexer_new(source)
    running := true
    while running {

        count := lexer.token_count

        if count > 0 {
            let (token_struct, toks_ref) := sys.list.get(lexer.tokens, count - 1)
            
            type := token_struct.type
            if type == TOKEN_EOF {
                running := false
            } else {
                lexer := lexer_scan(lexer)
            }
        } else {
            lexer := lexer_scan(lexer)
        }
    }
    return lexer.tokens
}

// --- TEST RUNNER ---

// Use string with newline char in it (not escaped)
src := "func let // comment \n if else class true false"
// print("Source: " + src)

tokens := lexer_tokenize(src)
print("Tokens Generated:")

// Print tokens pretty
let (cnt, toks_ref) := sys.len(tokens)
count := cnt
tokens := toks_ref
i := 0
while i < count {
    let (tok, _) := sys.list.get(tokens, i)
    print("Token:", tok)
    i := i + 1
}
